{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn | wc -l\n",
    "!pip install matplotlib | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/Tee\n",
      "1/Tank\n",
      "2/Dress\n",
      "3/Shorts\n",
      "4/Skirt\n",
      "5/Jumpsuit\n",
      "6/Sweater\n",
      "7/Blazer\n",
      "8/Striped\n",
      "9/Cardigan\n",
      "10/Blouse\n",
      "11/Romper\n",
      "12/Sweatpants\n",
      "13/Jacket\n"
     ]
    }
   ],
   "source": [
    "CLASS_LIST = [\"Tee\",\"Tank\",\"Dress\",\"Shorts\",\"Skirt\",\"Jumpsuit\",\"Sweater\",\"Blazer\",\"Striped\",\"Cardigan\",\"Blouse\",\"Romper\",\"Sweatpants\",\"Jacket\"]\n",
    "for idx, val in enumerate(CLASS_LIST):\n",
    "    print(\"%i/%s\" % (idx, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mdc20\\AppData\\Local\\Temp\\ipykernel_8752\\4263819878.py:6: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
    "os.chdir('C:\\\\Users\\\\mdc20')\n",
    "from keras.models import load_model\n",
    "model = load_model('best_model_14_class_lr@0.0001-1665971573.2646348.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done parsing through directories\n",
      "(289212, 3)\n",
      "Num classes #\n",
      "14\n",
      "Found 24638 validated image filenames belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "datasetdir = 'E:\\\\img_highres'\n",
    "os.chdir(datasetdir)\n",
    "\n",
    "from msilib.schema import Directory\n",
    "from random import shuffle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "BATCH_SIZE = 8\n",
    "IMG_SHAPE = (512,512)\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "datasetdir = 'E:\\\\img_highres'\n",
    "def constructImageClassDataFrame(shape, min_class_occurence = 1, class_list = None):\n",
    "    # Args: \n",
    "    #   Shape: Image shape (2D)\n",
    "    #   min_class_occurence: number of times class must occur in labeled dir in order to add it to the final sclass list\n",
    "    sub_dirs = [d for d in os.listdir(os.getcwd()) if os.path.isdir(os.path.join(datasetdir, d))]\n",
    "    \n",
    "    if class_list is None:\n",
    "        classes = defaultdict(lambda: 0)\n",
    "\n",
    "        for sub_dir in sub_dirs:\n",
    "            labels = sub_dir.split('_')\n",
    "            for label in labels:\n",
    "                classes[label] += 1\n",
    "        top_k_classes = [cls for cls in classes if classes[cls] >= min_class_occurence]\n",
    "        label_classes = top_k_classes    \n",
    "    else:\n",
    "        classes = class_list\n",
    "        label_classes = classes\n",
    "\n",
    "    \n",
    "    arr = [[labeled_dir, file] for labeled_dir in sub_dirs for file in os.listdir(labeled_dir) ]\n",
    "    print(\"Done parsing through directories\")\n",
    "    df = pd.DataFrame(data=arr, columns=[\"folder\",\"filename\"])\n",
    "\n",
    "\n",
    "\n",
    "    df['filename'] = df['folder'] + '/' + df['filename']\n",
    "    df['labels'] = df['folder'].apply(lambda x : [y for y in x.split('_') if y in label_classes] if len([y for y in x.split('_') if y in label_classes]) > 0 else None)\n",
    "\n",
    "    print(np.shape(df))\n",
    "    df = df[df.labels.notnull()]\n",
    "\n",
    "    return df, label_classes\n",
    "\n",
    "    \n",
    "\n",
    "def DataLoad(shape, preprocessing): \n",
    "    '''Create the training and validation datasets for \n",
    "    a given image shape.\n",
    "    '''\n",
    "    imgdatagen = ImageDataGenerator(\n",
    "        preprocessing_function = preprocessing,\n",
    "        horizontal_flip = True, \n",
    "        validation_split = 0.1,\n",
    "        rescale = 1.0/255, \n",
    "    )\n",
    "\n",
    "    height, width = shape\n",
    "\n",
    "    df, classes = constructImageClassDataFrame(shape, class_list=CLASS_LIST)\n",
    "\n",
    "    # Modify dataframe to only take images with references to classes\n",
    "\n",
    "    classes = list(classes)\n",
    "    print(\"Num classes #\")\n",
    "    print(len(classes)) \n",
    "    # for subdir\n",
    "\n",
    "    # train_dataset = imgdatagen.flow_from_dataframe(\n",
    "    #     dataframe = df,\n",
    "    #     directory = datasetdir,\n",
    "    #     x_col=\"filename\",\n",
    "    #     y_col=\"labels\",\n",
    "    #     batch_size = batch_size,\n",
    "    #     seed = 116,\n",
    "    #     shuffle = True,\n",
    "    #     class_mode=\"categorical\",\n",
    "    #     classes = classes,\n",
    "    #     subset = 'training'\n",
    "    # )\n",
    "    val_dataset = imgdatagen.flow_from_dataframe(\n",
    "        dataframe = df,\n",
    "        directory = datasetdir,\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"labels\",\n",
    "        batch_size = BATCH_SIZE,\n",
    "        seed = 42,\n",
    "        target_size = IMG_SHAPE,\n",
    "        shuffle = False,\n",
    "        class_mode=\"categorical\",\n",
    "        classes = classes,\n",
    "        subset = 'validation'\n",
    "    )\n",
    "\n",
    "\n",
    "    return 0, val_dataset, df\n",
    "\n",
    "_, val_dataset, df = DataLoad((512,512), preprocessing=preprocess_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as skm\n",
    "def get_indices_from_keras_generator(gen, batch_size):\n",
    "    \"\"\"\n",
    "    Given a keras data generator, it returns the indices and the filepaths\n",
    "    corresponding the current batch. \n",
    "    :param gen: keras generator.\n",
    "    :param batch_size: size of the last batch generated.\n",
    "    :return: tuple with indices and filenames\n",
    "    \"\"\"\n",
    "\n",
    "    idx_left = (gen.batch_index - 1) * batch_size\n",
    "    idx_right = idx_left + gen.batch_size if idx_left >= 0 else None\n",
    "    indices = gen.index_array[idx_left:idx_right]\n",
    "    filenames = [gen.filenames[i] for i in indices]\n",
    "    return indices, filenames\n",
    "from io import StringIO\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def get_confusion_matrix(y_test, y_hat, class_list, threshold = 0.5):\n",
    "    # y_test is of form E = [e..e] where |E|=12 and e\\in {0,1}\n",
    "    # y_hat is of form P = [x..x] where 0 < x < 1 where P_x indicates probability of feature\n",
    "    y_hat = np.where(y_hat > threshold,1,0)\n",
    "    cm = confusion_matrix(y_test,y_hat,labels=class_list)\n",
    "    return cm\n",
    "# X_test, y_test = next(val_dataset)\n",
    "# y_hat = model.predict(X_test)\n",
    "\n",
    "class_list=CLASS_LIST\n",
    "\n",
    "conf_matrix = np.array(np.zeros((14,2,2)))\n",
    "print(np.shape(conf_matrix))\n",
    "predicted_class_df = pd.DataFrame(columns=[\"filepath\",\"class\"])\n",
    "MAX_ITERS = 500\n",
    "it = 0\n",
    "results_df = pd.DataFrame(columns=['class_identifier','precision','recall','f1-score','support','batch_number'],dtype=float)\n",
    "for X_test,y_test in val_dataset:\n",
    "    if it > MAX_ITERS:\n",
    "        break\n",
    "    if it % 10 == 0:\n",
    "        print(\"%i/%i\" % (it, MAX_ITERS))\n",
    "    it = it + 1\n",
    "    indices, filenames = get_indices_from_keras_generator(val_dataset,batch_size=4)\n",
    "    y_hat = model.predict(X_test,verbose=0)\n",
    "    y_hat = np.where(y_hat > 0.5,1,0)\n",
    "    _\n",
    "    # y_test_labels = np.argmax(y_test,axis=1)\n",
    "    # y_hat_labels = np.argmax(y_hat, axis=1)\n",
    "\n",
    "    # y_test_cm_input = [class_list[idx] for idx in y_test_labels]\n",
    "    # y_hat_cm_input = [class_list[idx] for idx in y_hat_labels]\n",
    "    cm = multilabel_confusion_matrix(y_test,y_hat,labels=range(0,14))\n",
    "    # cm = get_confusion_matrix(y_test, y_hat, class_list=class_list)\n",
    "    conf_matrix = np.add(conf_matrix, cm)\n",
    "    # report = classification_report(y_test,y_hat,zero_division=0)\n",
    "    # print(report)\n",
    "    # rep_arr = report.split('\\n')\n",
    "    # rep_header = rep_arr[0].split()\n",
    "    # rep_body = [row.split() for row in rep_arr[1:25]][1:]\n",
    "# \n",
    "    # results_df.concat(rep_body)\n",
    "    # df = pd.DataFrame(data=rep_body, columns=['class_identifier','precision','recall','f1-score','support'], index=class_list,dtype=float)\n",
    "    # df['batch_number'] = it\n",
    "    # results_df = pd.concat([results_df, df])\n",
    "\n",
    "for class_name, conf_matrix in zip(class_list,conf_matrix):\n",
    "    print(class_name)\n",
    "    print(conf_matrix)\n",
    "# results_df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# results_df = results_df.drop(results_df[results_df.support == '0'].index)\n",
    "\n",
    "# filtered_df = results_df[results_df.support != '0']\n",
    "print(len(results_df))\n",
    "# filtered_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3080/3080 [==============================] - 717s 227ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24638, 14)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24638, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=range(0,14), sparse_output=False)\n",
    "y_true = val_dataset.classes\n",
    "y_true_ohe = mlb.fit_transform(y_true)\n",
    "print(np.shape(y_true_ohe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarizeOutput(y_pred, threshold=0.5):\n",
    "    y_pred_bin = np.where(y_pred > threshold,1,0)\n",
    "    return y_pred_bin\n",
    "y_pred_bin = binarizeOutput(y_pred, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(y_pred_bin))\n",
    "y_sum = np.sum(y_pred_bin,axis=1)\n",
    "print(np.shape(y_sum))\n",
    "y_sum_peek = y_sum[100:200]\n",
    "print(y_sum_peek)\n",
    "print(np.sum(y_sum))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Classes\n",
    "0/Tee\n",
    "1/Tank\n",
    "2/Dress\n",
    "3/Shorts\n",
    "4/Skirt\n",
    "5/Hoodie\n",
    "6/Jumpsuit\n",
    "7/Sweater\n",
    "8/Knit\n",
    "9/Plaid\n",
    "10/Blazer\n",
    "11/Striped\n",
    "12/Cardigan\n",
    "13/Blouse\n",
    "14/Dotted\n",
    "15/Romper\n",
    "16/Open-Back\n",
    "17/Sweatpants\n",
    "18/Leopard\n",
    "19/Jacket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67      4369\n",
      "           1       0.68      0.45      0.54      1976\n",
      "           2       0.76      0.87      0.81      7581\n",
      "           3       0.64      0.59      0.62      1687\n",
      "           4       0.69      0.35      0.47      1425\n",
      "           5       0.69      0.37      0.48       860\n",
      "           6       0.54      0.21      0.30       996\n",
      "           7       0.45      0.20      0.28       318\n",
      "           8       0.37      0.21      0.26       685\n",
      "           9       0.50      0.12      0.20       639\n",
      "          10       0.46      0.49      0.47      2627\n",
      "          11       0.61      0.26      0.37      1263\n",
      "          12       0.74      0.31      0.43       222\n",
      "          13       0.61      0.12      0.20       520\n",
      "\n",
      "   micro avg       0.67      0.58      0.62     25168\n",
      "   macro avg       0.60      0.37      0.44     25168\n",
      "weighted avg       0.65      0.58      0.59     25168\n",
      " samples avg       0.56      0.58      0.57     25168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "SEARCH_THRESHOLD = False\n",
    "t = 0.4\n",
    "for threshold in [0.05, 0.10, 0.15, 0.20, 0.225, 0.25, 0.275, 0.3, 0.35, 0.4,0.45, 0.5]:\n",
    "    if(not SEARCH_THRESHOLD): break\n",
    "    y_pred_bin = binarizeOutput(y_pred, threshold=threshold)    \n",
    "\n",
    "    c_report = classification_report(y_true=y_true_ohe, y_pred=y_pred_bin,zero_division=False)\n",
    "    \n",
    "    reports=c_report.split('\\n')\n",
    "    print(\"threshold:%f\", threshold)\n",
    "    print(reports[17])\n",
    "\n",
    "\n",
    "y_pred_bin = binarizeOutput(y_pred, threshold=t)    \n",
    "c_report = classification_report(y_true=y_true_ohe, y_pred=y_pred_bin,zero_division=False)\n",
    "print(c_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# print(y_test)\n",
    "y_test_labels = np.argmax(y_test,axis=1)\n",
    "y_hat_labels = np.argmax(y_hat, axis=1)\n",
    "\n",
    "# print(np.shape(y_test_labels))\n",
    "# print(np.shape(y_hat_labels))\n",
    "# cm = confusion_matrix(y_test_labels,y_hat_labels,labels=class_list)\n",
    "df_cm = pd.DataFrame(conf_matrix, index = class_list, columns = class_list)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(val_dataset.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices, filenames = get_indices_from_keras_generator(val_dataset, 1024)\n",
    "print(np.shape(indices))\n",
    "print(np.shape(filenames))\n",
    "print(indices[0])\n",
    "print(filenames[0:4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('gpu3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b689eaeee660c5f4e77cc22f066091ae7dcd8b72ca1cd0b8aadc4281bbfd7404"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
