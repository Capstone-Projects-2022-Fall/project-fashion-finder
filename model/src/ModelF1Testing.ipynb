{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn | wc -l\n",
    "!pip install matplotlib | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/Tee\n",
      "1/Tank\n",
      "2/Dress\n",
      "3/Shorts\n",
      "4/Skirt\n",
      "5/Hoodie\n",
      "6/Jumpsuit\n",
      "7/Sweater\n",
      "8/Blazer\n",
      "9/Striped\n",
      "10/Cardigan\n",
      "11/Blouse\n",
      "12/Jacket\n",
      "13/Jeans\n",
      "14/Maxi\n",
      "15/Floral\n",
      "16/Denim\n",
      "17/Sweatshorts\n",
      "18/Polka\n",
      "19/Shawl\n",
      "20/Bodycon\n"
     ]
    }
   ],
   "source": [
    "CLASS_LIST_29 = [\"Tee\",\"Tank\",\"Dress\",\"Shorts\",\"Skirt\",\"Jumpsuit\",\"Sweater\",\"Blazer\",\"Striped\",\"Cardigan\",\"Blouse\",\"Romper\",\"Sweatpants\",\"Jacket\", \"Jeans\", \"Poncho\", \"Button-Down\", \"Pencil\", \"Maxi\",  \"Floral\", \"Pleated\", \"Mesh\",\"Bandana\", \"Tie-Dye\", \"Culottes\", \"Embroidered\", \"Kimono\",\"Chevron\", \"Buttoned\"]\n",
    "CLASS_LIST_35 = [\"Tee\",\"Tank\",\"Dress\",\"Shirt\",\"Shorts\",\"Skirt\",\"Hoodie\",\"Jumpsuit\",\"Cargo\",\"Turtleneck\",\"Sweater\",\"Plaid\",\"Blazer\",\"Striped\",\"Cardigan\",\"Blouse\",\"Romper\",\"Jacket\",\"Jeans\",\"Maxi\",\"Floral\",\"Denim\",\"Trench\",\"Baroque\",\"Ornate\",\"Belted\",\"Jersey\",\"Sweatshorts\",\"PJ\",\"Parka\",\"Polka\",\"Beaded\",\"Shawl\",\"Bodycon\",\"Abstract\"]\n",
    "CLASS_LIST_21 = [\"Tee\",\"Tank\",\"Dress\",\"Shorts\",\"Skirt\",\"Hoodie\",\"Jumpsuit\",\"Sweater\",\"Blazer\",\"Striped\",\"Cardigan\",\"Blouse\",\"Jacket\",\"Jeans\",\"Maxi\",\"Floral\",\"Denim\",\"Sweatshorts\",\"Polka\",\"Shawl\",\"Bodycon\"]\n",
    "CLASS_LIST = CLASS_LIST_21\n",
    "\n",
    "for idx, val in enumerate(CLASS_LIST):\n",
    "    print(\"%i/%s\" % (idx, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as tfb\n",
    "import numpy as np\n",
    "\n",
    "POS_WEIGHT = 10  # multiplier for positive targets, needs to be tuned\n",
    "\n",
    "def weighted_binary_crossentropy(target, output):\n",
    "    \"\"\"\n",
    "    Weighted binary crossentropy between an output tensor \n",
    "    and a target tensor. POS_WEIGHT is used as a multiplier \n",
    "    for the positive targets.\n",
    "\n",
    "    Combination of the following functions:\n",
    "    * keras.losses.binary_crossentropy\n",
    "    * keras.backend.tensorflow_backend.binary_crossentropy\n",
    "    * tf.nn.weighted_cross_entropy_with_logits\n",
    "    \"\"\"\n",
    "    # transform back to logits\n",
    "    _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "    output = tfb.log(output / (1 - output))\n",
    "    # compute weighted loss\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(labels=target,\n",
    "                                                    logits=output,\n",
    "                                                    pos_weight=POS_WEIGHT)\n",
    "    return tf.reduce_mean(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click below to import f1 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
    "os.chdir('C:\\\\Users\\\\mdc20')\n",
    "from keras.models import load_model\n",
    "model = load_model('best_model_21_class_lr@0.002-1666761586.9275644.hdf5',custom_objects={'weighted_binary_crossentropy':weighted_binary_crossentropy, 'f1_loss':f1_loss, 'f1':f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done parsing through directories\n",
      "(289212, 3)\n",
      "Num classes #\n",
      "21\n",
      "Found 126798 validated image filenames belonging to 21 classes.\n"
     ]
    }
   ],
   "source": [
    "datasetdir = 'E:\\\\img_highres'\n",
    "os.chdir(datasetdir)\n",
    "\n",
    "from msilib.schema import Directory\n",
    "from random import shuffle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "BATCH_SIZE = 16\n",
    "IMG_SHAPE = (224,224)\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "datasetdir = 'E:\\\\img_highres'\n",
    "def constructImageClassDataFrame(shape, min_class_occurence = 1, class_list = None):\n",
    "    # Args: \n",
    "    #   Shape: Image shape (2D)\n",
    "    #   min_class_occurence: number of times class must occur in labeled dir in order to add it to the final sclass list\n",
    "    sub_dirs = [d for d in os.listdir(os.getcwd()) if os.path.isdir(os.path.join(datasetdir, d))]\n",
    "    \n",
    "    if class_list is None:\n",
    "        classes = defaultdict(lambda: 0)\n",
    "\n",
    "        for sub_dir in sub_dirs:\n",
    "            labels = sub_dir.split('_')\n",
    "            for label in labels:\n",
    "                classes[label] += 1\n",
    "        top_k_classes = [cls for cls in classes if classes[cls] >= min_class_occurence]\n",
    "        label_classes = top_k_classes    \n",
    "    else:\n",
    "        classes = class_list\n",
    "        label_classes = classes\n",
    "\n",
    "    \n",
    "    arr = [[labeled_dir, file] for labeled_dir in sub_dirs for file in os.listdir(labeled_dir) ]\n",
    "    print(\"Done parsing through directories\")\n",
    "    df = pd.DataFrame(data=arr, columns=[\"folder\",\"filename\"])\n",
    "\n",
    "\n",
    "\n",
    "    df['filename'] = df['folder'] + '/' + df['filename']\n",
    "    df['labels'] = df['folder'].apply(lambda x : [y for y in x.split('_') if y in label_classes] if len([y for y in x.split('_') if y in label_classes]) > 0 else None)\n",
    "\n",
    "    print(np.shape(df))\n",
    "    df = df[df.labels.notnull()]\n",
    "\n",
    "    return df, label_classes\n",
    "\n",
    "    \n",
    "\n",
    "def DataLoad(shape, preprocessing): \n",
    "    '''Create the training and validation datasets for \n",
    "    a given image shape.\n",
    "    '''\n",
    "    imgdatagen = ImageDataGenerator(\n",
    "        preprocessing_function = preprocessing,\n",
    "        horizontal_flip = True, \n",
    "        validation_split = 0.5,\n",
    "        rescale = 1.0/255, \n",
    "    )\n",
    "\n",
    "    height, width = shape\n",
    "\n",
    "    df, classes = constructImageClassDataFrame(shape, class_list=CLASS_LIST)\n",
    "\n",
    "    # Modify dataframe to only take images with references to classes\n",
    "\n",
    "    classes = list(classes)\n",
    "    print(\"Num classes #\")\n",
    "    print(len(classes)) \n",
    "    # for subdir\n",
    "\n",
    "    # train_dataset = imgdatagen.flow_from_dataframe(\n",
    "    #     dataframe = df,\n",
    "    #     directory = datasetdir,\n",
    "    #     x_col=\"filename\",\n",
    "    #     y_col=\"labels\",\n",
    "    #     batch_size = batch_size,\n",
    "    #     seed = 116,\n",
    "    #     shuffle = True,\n",
    "    #     class_mode=\"categorical\",\n",
    "    #     classes = classes,\n",
    "    #     subset = 'training'\n",
    "    # )\n",
    "    val_dataset = imgdatagen.flow_from_dataframe(\n",
    "        dataframe = df,\n",
    "        directory = datasetdir,\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"labels\",\n",
    "        batch_size = BATCH_SIZE,\n",
    "        seed = 42,\n",
    "        target_size = IMG_SHAPE,\n",
    "        shuffle = True,\n",
    "        class_mode=\"categorical\",\n",
    "        classes = classes,\n",
    "        subset = 'validation'\n",
    "    )\n",
    "\n",
    "\n",
    "    return 0, val_dataset, df\n",
    "\n",
    "_, val_dataset, df = DataLoad((224,224), preprocessing=preprocess_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 2, 2)\n",
      "0/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mUbuntu-20.04\\home\\mdc20\\repos\\personal\\project-fashion-finder\\model\\src\\ModelF1Testing.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu-20.04/home/mdc20/repos/personal/project-fashion-finder/model/src/ModelF1Testing.ipynb#W4sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m it \u001b[39m=\u001b[39m it \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu-20.04/home/mdc20/repos/personal/project-fashion-finder/model/src/ModelF1Testing.ipynb#W4sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m indices, filenames \u001b[39m=\u001b[39m get_indices_from_keras_generator(val_dataset,batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell://wsl.localhost/Ubuntu-20.04/home/mdc20/repos/personal/project-fashion-finder/model/src/ModelF1Testing.ipynb#W4sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m y_hat \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_test,verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu-20.04/home/mdc20/repos/personal/project-fashion-finder/model/src/ModelF1Testing.ipynb#W4sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m y_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(y_hat \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu-20.04/home/mdc20/repos/personal/project-fashion-finder/model/src/ModelF1Testing.ipynb#W4sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m _\n",
      "File \u001b[1;32mc:\\Users\\mdc20\\anaconda3\\envs\\gpu3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mdc20\\anaconda3\\envs\\gpu3\\lib\\site-packages\\keras\\engine\\training.py:2253\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2251\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   2252\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2253\u001b[0m     tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[0;32m   2254\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   2255\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\mdc20\\anaconda3\\envs\\gpu3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mdc20\\anaconda3\\envs\\gpu3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\mdc20\\anaconda3\\envs\\gpu3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:986\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    982\u001b[0m   _, _, filtered_flat_args \u001b[39m=\u001b[39m (\n\u001b[0;32m    983\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn\u001b[39m.\u001b[39m_function_spec\u001b[39m.\u001b[39mcanonicalize_function_inputs(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    984\u001b[0m           \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[0;32m    985\u001b[0m   \u001b[39m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_concrete_stateful_fn\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    987\u001b[0m       filtered_flat_args, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_concrete_stateful_fn\u001b[39m.\u001b[39;49mcaptured_inputs)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn_with_cond\u001b[39m(inner_args, inner_kwds, inner_filtered_flat_args):\n\u001b[0;32m    990\u001b[0m   \u001b[39m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mdc20\\anaconda3\\envs\\gpu3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\mdc20\\anaconda3\\envs\\gpu3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\mdc20\\anaconda3\\envs\\gpu3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as skm\n",
    "def get_indices_from_keras_generator(gen, batch_size):\n",
    "    \"\"\"\n",
    "    Given a keras data generator, it returns the indices and the filepaths\n",
    "    corresponding the current batch. \n",
    "    :param gen: keras generator.\n",
    "    :param batch_size: size of the last batch generated.\n",
    "    :return: tuple with indices and filenames\n",
    "    \"\"\"\n",
    "\n",
    "    idx_left = (gen.batch_index - 1) * batch_size\n",
    "    idx_right = idx_left + gen.batch_size if idx_left >= 0 else None\n",
    "    indices = gen.index_array[idx_left:idx_right]\n",
    "    filenames = [gen.filenames[i] for i in indices]\n",
    "    return indices, filenames\n",
    "from io import StringIO\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def get_confusion_matrix(y_test, y_hat, class_list, threshold = 0.5):\n",
    "    # y_test is of form E = [e..e] where |E|=12 and e\\in {0,1}\n",
    "    # y_hat is of form P = [x..x] where 0 < x < 1 where P_x indicates probability of feature\n",
    "    y_hat = np.where(y_hat > threshold,1,0)\n",
    "    cm = confusion_matrix(y_test,y_hat,labels=class_list)\n",
    "    return cm\n",
    "# X_test, y_test = next(val_dataset)\n",
    "# y_hat = model.predict(X_test)\n",
    "\n",
    "class_list=CLASS_LIST\n",
    "\n",
    "conf_matrix = np.array(np.zeros((14,2,2)))\n",
    "print(np.shape(conf_matrix))\n",
    "predicted_class_df = pd.DataFrame(columns=[\"filepath\",\"class\"])\n",
    "MAX_ITERS = 500\n",
    "it = 0\n",
    "results_df = pd.DataFrame(columns=['class_identifier','precision','recall','f1-score','support','batch_number'],dtype=float)\n",
    "for X_test,y_test in val_dataset:\n",
    "    if it > MAX_ITERS:\n",
    "        break\n",
    "    if it % 10 == 0:\n",
    "        print(\"%i/%i\" % (it, MAX_ITERS))\n",
    "    it = it + 1\n",
    "    indices, filenames = get_indices_from_keras_generator(val_dataset,batch_size=4)\n",
    "    y_hat = model.predict(X_test,verbose=0)\n",
    "    y_hat = np.where(y_hat > 0.5,1,0)\n",
    "    _\n",
    "    # y_test_labels = np.argmax(y_test,axis=1)\n",
    "    # y_hat_labels = np.argmax(y_hat, axis=1)\n",
    "\n",
    "    # y_test_cm_input = [class_list[idx] for idx in y_test_labels]\n",
    "    # y_hat_cm_input = [class_list[idx] for idx in y_hat_labels]\n",
    "    cm = multilabel_confusion_matrix(y_test,y_hat,labels=range(0,14))\n",
    "    # cm = get_confusion_matrix(y_test, y_hat, class_list=class_list)\n",
    "    conf_matrix = np.add(conf_matrix, cm)\n",
    "    # report = classification_report(y_test,y_hat,zero_division=0)\n",
    "    # print(report)\n",
    "    # rep_arr = report.split('\\n')\n",
    "    # rep_header = rep_arr[0].split()\n",
    "    # rep_body = [row.split() for row in rep_arr[1:25]][1:]\n",
    "# \n",
    "    # results_df.concat(rep_body)\n",
    "    # df = pd.DataFrame(data=rep_body, columns=['class_identifier','precision','recall','f1-score','support'], index=class_list,dtype=float)\n",
    "    # df['batch_number'] = it\n",
    "    # results_df = pd.concat([results_df, df])\n",
    "\n",
    "for class_name, conf_matrix in zip(class_list,conf_matrix):\n",
    "    print(class_name)\n",
    "    print(conf_matrix)\n",
    "# results_df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# results_df = results_df.drop(results_df[results_df.support == '0'].index)\n",
    "\n",
    "# filtered_df = results_df[results_df.support != '0']\n",
    "print(len(results_df))\n",
    "# filtered_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7925/7925 [==============================] - 1069s 134ms/step\n"
     ]
    }
   ],
   "source": [
    "# Protect against DecompressionBomb Gating halting the training\n",
    "import PIL\n",
    "PIL.Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "\n",
    "y_pred = model.predict(val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126798, 21)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50719, 21)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=range(0,len(CLASS_LIST)), sparse_output=False)\n",
    "y_true = val_dataset.classes\n",
    "y_true_ohe = mlb.fit_transform(y_true)\n",
    "print(np.shape(y_true_ohe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarizeOutput(y_pred, threshold=0.5):\n",
    "    if isinstance(threshold, list):\n",
    "        if (len(threshold) == len(y_pred[0])):\n",
    "            compare_to = [threshold for i in range(0, len(y_pred))]\n",
    "            y_pred_bin  = y_pred > compare_to\n",
    "    else:\n",
    "        y_pred_bin = np.where(y_pred > threshold,1,0)\n",
    "    return y_pred_bin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_threshold = [0.35, 0.25, 0.45, 0.2, 0.15, 0.075, 0.15, 0.15, 0.2, 0.1, 0.2, 0.05, 0.05, 0.275, 0.15, 1.00, 1.00, 1.00, 0.2, 0.15, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00]\n",
    "y_pred_bin = binarizeOutput(y_pred, threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126798, 21)\n",
      "(126798,)\n",
      "[1 2 1 3 1 2 1 1 1 0 1 2 1 1 2 2 2 2 2 1 2 2 2 2 2 1 1 1 0 3 2 2 1 2 1 1 1\n",
      " 1 1 1 1 1 2 1 2 0 2 2 1 1 1 1 1 1 1 2 1 1 1 1 0 0 1 1 1 1 2 2 3 2 1 2 1 2\n",
      " 2 1 3 2 1 1 1 1 1 2 2 2 2 1 1 1 1 1 1 2 2 2 3 1 0 1]\n",
      "174805\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(y_pred_bin))\n",
    "y_sum = np.sum(y_pred_bin,axis=1)\n",
    "print(np.shape(y_sum))\n",
    "y_sum_peek = y_sum[100:200]\n",
    "print(y_sum_peek)\n",
    "print(np.sum(y_sum))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Classes\n",
    "0/Tee\n",
    "1/Tank\n",
    "2/Dress\n",
    "3/Shorts\n",
    "4/Skirt\n",
    "5/Jumpsuit\n",
    "6/Sweater\n",
    "7/Blazer\n",
    "8/Striped\n",
    "9/Cardigan\n",
    "10/Blouse\n",
    "11/Romper\n",
    "12/Sweatpants\n",
    "13/Jacket\n",
    "14/Jeans\n",
    "15/Poncho\n",
    "16/Button-Down\n",
    "17/Pencil\n",
    "18/Maxi\n",
    "19/Floral\n",
    "20/Pleated\n",
    "21/Mesh\n",
    "22/Bandana\n",
    "23/Tie-Dye\n",
    "24/Culottes\n",
    "25/Embroidered\n",
    "26/Kimono\n",
    "27/Chevron\n",
    "28/Buttoned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "SEARCH_THRESHOLD = True\n",
    "t = 0.4\n",
    "print(\"              precision    recall  f1-score   support\")\n",
    "for threshold in [0.05, 0.10, 0.15, 0.20, 0.225, 0.25, 0.275, 0.3, 0.35, 0.4,0.45, 0.5]:\n",
    "    if(not SEARCH_THRESHOLD): break\n",
    "    y_pred_bin = binarizeOutput(y_pred, threshold=threshold)    \n",
    "\n",
    "    c_report = classification_report(y_true=y_true_ohe, y_pred=y_pred_bin,zero_division=False)\n",
    "    # sample_avg =c_report.split('\\n')[35]\n",
    "    # print(c)\n",
    "    # print(sample_avg)\n",
    "    print(\"threshold:%f\", threshold)\n",
    "    print(c_report)\n",
    "\n",
    "\n",
    "# y_pred_bin = binarizeOutput(y_pred, threshold=test_threshold)    \n",
    "# c_report = classification_report(y_true=y_true_ohe, y_pred=y_pred_bin,zero_division=False)\n",
    "# print(c_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(14, 2, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mUbuntu-20.04\\home\\mdc20\\repos\\personal\\project-fashion-finder\\model\\src\\ModelF1Testing.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu-20.04/home/mdc20/repos/personal/project-fashion-finder/model/src/ModelF1Testing.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y_hat_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(y_hat, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell://wsl.localhost/Ubuntu-20.04/home/mdc20/repos/personal/project-fashion-finder/model/src/ModelF1Testing.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# print(np.shape(y_test_labels))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu-20.04/home/mdc20/repos/personal/project-fashion-finder/model/src/ModelF1Testing.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# print(np.shape(y_hat_labels))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu-20.04/home/mdc20/repos/personal/project-fashion-finder/model/src/ModelF1Testing.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# cm = confusion_matrix(y_test_labels,y_hat_labels,labels=class_list)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell://wsl.localhost/Ubuntu-20.04/home/mdc20/repos/personal/project-fashion-finder/model/src/ModelF1Testing.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m df_cm \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(conf_matrix, index \u001b[39m=\u001b[39;49m class_list, columns \u001b[39m=\u001b[39;49m class_list)\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu-20.04/home/mdc20/repos/personal/project-fashion-finder/model/src/ModelF1Testing.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize \u001b[39m=\u001b[39m (\u001b[39m10\u001b[39m,\u001b[39m7\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell://wsl.localhost/Ubuntu-20.04/home/mdc20/repos/personal/project-fashion-finder/model/src/ModelF1Testing.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m sns\u001b[39m.\u001b[39mheatmap(df_cm,annot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\mdc20\\anaconda3\\envs\\gpu3\\lib\\site-packages\\pandas\\core\\frame.py:721\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    711\u001b[0m         mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    712\u001b[0m             \u001b[39m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    713\u001b[0m             \u001b[39m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    718\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    719\u001b[0m         )\n\u001b[0;32m    720\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 721\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    722\u001b[0m             data,\n\u001b[0;32m    723\u001b[0m             index,\n\u001b[0;32m    724\u001b[0m             columns,\n\u001b[0;32m    725\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    726\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    727\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[0;32m    728\u001b[0m         )\n\u001b[0;32m    730\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    731\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\mdc20\\anaconda3\\envs\\gpu3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:330\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    325\u001b[0m         values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m    327\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    328\u001b[0m     \u001b[39m# by definition an array here\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[39m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m     values \u001b[39m=\u001b[39m _prep_ndarraylike(values, copy\u001b[39m=\u001b[39;49mcopy_on_sanitize)\n\u001b[0;32m    332\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dtype_equal(values\u001b[39m.\u001b[39mdtype, dtype):\n\u001b[0;32m    333\u001b[0m     \u001b[39m# GH#40110 see similar check inside sanitize_array\u001b[39;00m\n\u001b[0;32m    334\u001b[0m     rcf \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m (is_integer_dtype(dtype) \u001b[39mand\u001b[39;00m values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mdc20\\anaconda3\\envs\\gpu3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:584\u001b[0m, in \u001b[0;36m_prep_ndarraylike\u001b[1;34m(values, copy)\u001b[0m\n\u001b[0;32m    582\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mreshape((values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\n\u001b[0;32m    583\u001b[0m \u001b[39melif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> 584\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMust pass 2-d input. shape=\u001b[39m\u001b[39m{\u001b[39;00mvalues\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    586\u001b[0m \u001b[39mreturn\u001b[39;00m values\n",
      "\u001b[1;31mValueError\u001b[0m: Must pass 2-d input. shape=(14, 2, 2)"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# print(y_test)\n",
    "y_test_labels = np.argmax(y_test,axis=1)\n",
    "y_hat_labels = np.argmax(y_hat, axis=1)\n",
    "\n",
    "# print(np.shape(y_test_labels))\n",
    "# print(np.shape(y_hat_labels))\n",
    "# cm = confusion_matrix(y_test_labels,y_hat_labels,labels=class_list)\n",
    "df_cm = pd.DataFrame(conf_matrix, index = class_list, columns = class_list)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(val_dataset.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices, filenames = get_indices_from_keras_generator(val_dataset, 1024)\n",
    "print(np.shape(indices))\n",
    "print(np.shape(filenames))\n",
    "print(indices[0])\n",
    "print(filenames[0:4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('gpu3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b689eaeee660c5f4e77cc22f066091ae7dcd8b72ca1cd0b8aadc4281bbfd7404"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
